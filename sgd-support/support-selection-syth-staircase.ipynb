{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.nn import ReLU\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from typing import Optional\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyperparams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_init = 0\n",
    "hiddens = [15, 15, 15]\n",
    "n_iters_gd = 20_000\n",
    "n_iters_sgd = 200_000\n",
    "n_iters_gdwd = 5_000\n",
    "n_trajs = 1\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# util functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seedall(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5000 # number of samples.\n",
    "d = 15 # dimension of the data.\n",
    "r = 5 # end-index of relevant features.\n",
    "\n",
    "seedall(seed_init)\n",
    "\n",
    "# Sample raw data at first.\n",
    "x = np.random.randn(n, d)\n",
    "\n",
    "# Prepare the data as per assumptions.\n",
    "idx_to_center = range(r, d)\n",
    "means = np.mean(x[:, idx_to_center], axis=0)\n",
    "x[:, idx_to_center] -= means\n",
    "\n",
    "# Define generating function.\n",
    "W = np.array([1] * d) # np.random.randn(d)\n",
    "W[r:] = 0 # Set to zero irrelevant features.\n",
    "\n",
    "# Gaussian noise.\n",
    "eps = 0.001 * np.random.randn(n) * 0\n",
    "\n",
    "def generate_weighted_staircase_labels(x, W, r):\n",
    "    y = np.zeros(x.shape[0])\n",
    "    for i in range(r):\n",
    "        y += W[i] * np.power(x[:, i], i + 1)\n",
    "    return y\n",
    "\n",
    "# Generate labels.\n",
    "y1 = (generate_weighted_staircase_labels(x, W, r) + eps).reshape(-1, 1)\n",
    "y2 = (generate_weighted_staircase_labels(x, W, r) - eps).reshape(-1, 1)\n",
    "\n",
    "x = np.concatenate([x, x], axis=0)\n",
    "y = np.concatenate([y1, y2], axis=0)\n",
    "\n",
    "# Map to PyTorch tensors.\n",
    "xt = torch.from_numpy(x).float()\n",
    "yt = torch.from_numpy(y).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that data features are not correlated.\n",
    "cov = xt.T @ xt\n",
    "plt.imshow(cov, cmap='hot', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define our proper model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        bias: bool = True,\n",
    "        act_func: Optional[nn.Module] = None,\n",
    "        skip_connections: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        modules = OrderedDict()\n",
    "        modules[\"linear\"] = nn.Linear(in_features=in_features, out_features=out_features, bias=bias)\n",
    "        modules[\"activation\"] = act_func if act_func is not None else nn.Identity()\n",
    "        self.linear_act_block = nn.Sequential(modules)\n",
    "        self.skip_connections = skip_connections\n",
    "        if skip_connections and in_features != out_features:\n",
    "            self.adjust_dim = nn.Linear(in_features, out_features, bias=False)\n",
    "        else:\n",
    "            self.adjust_dim = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.skip_connections:\n",
    "            x_adj = self.adjust_dim(x) if self.adjust_dim is not None else x\n",
    "            return x_adj + self.linear_act_block(x)\n",
    "        return self.linear_act_block(x)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        hidden_features: list[int],\n",
    "        out_features: int,\n",
    "        bias: bool = True,\n",
    "        act_func: Optional[nn.Module] = None,\n",
    "        skip_connections: bool = False,\n",
    "        init_method: str = \"he_normal\"\n",
    "    ) -> None:\n",
    "        super(MLP, self).__init__()\n",
    "        assert len(hidden_features) >= 1\n",
    "\n",
    "        modules = OrderedDict()\n",
    "        hidden_dims = [in_features] + hidden_features + [out_features]\n",
    "\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules[f\"layer_{i}\"] = LinearBlock(\n",
    "                in_features=hidden_dims[i],\n",
    "                out_features=hidden_dims[i + 1],\n",
    "                skip_connections=skip_connections,\n",
    "                # Use the activation function for all layers but last fc\n",
    "                act_func=act_func if i < len(hidden_dims) - 2 else None,\n",
    "                bias=bias,\n",
    "            )\n",
    "\n",
    "        self.layers = nn.Sequential(modules)\n",
    "        self.init_weights(self.layers, init_method)\n",
    "        self.penultimate = None\n",
    "        self.set_init()\n",
    "\n",
    "    def init_weights(self, module, init_method):\n",
    "        for child in module.children():\n",
    "            if isinstance(child, nn.Linear):\n",
    "                self.apply_init(child, init_method)\n",
    "            else:\n",
    "                self.init_weights(child, init_method)\n",
    "\n",
    "    def apply_init(self, linear_layer, init_method):\n",
    "        if init_method == \"xavier_uniform\":\n",
    "            nn.init.xavier_uniform_(linear_layer.weight)\n",
    "        elif init_method == \"xavier_normal\":\n",
    "            nn.init.xavier_normal_(linear_layer.weight)\n",
    "        elif init_method == \"he_normal\":\n",
    "            nn.init.kaiming_normal_(linear_layer.weight, nonlinearity=\"relu\")\n",
    "        elif init_method == \"he_uniform\":\n",
    "            nn.init.kaiming_uniform_(linear_layer.weight, nonlinearity=\"relu\")\n",
    "\n",
    "    def embeddings(self, x):\n",
    "        activations = []\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = layer(x)\n",
    "            activations.append(x)\n",
    "        return activations\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.penultimate = self.layers[:-1](x)\n",
    "        logits = self.layers[-1](self.penultimate)\n",
    "        return logits\n",
    "    \n",
    "    def set_init(self):\n",
    "        self.W1_init = self.layers.layer_0.linear_act_block.linear.weight.detach().clone().numpy()\n",
    "        self.W2_init = self.layers.layer_1.linear_act_block.linear.weight.detach().clone().numpy()\n",
    "        self.W3_init = self.layers.layer_2.linear_act_block.linear.weight.detach().clone().numpy()\n",
    "        self.u_init, self.s_init, self.vh_init = np.linalg.svd(self.W1_init)\n",
    "        self.W1_init_svd = self.W1_init @ self.vh_init.T\n",
    "\n",
    "    def set_post(self):\n",
    "        self.W1_post = self.layers.layer_0.linear_act_block.linear.weight.detach().clone().numpy()\n",
    "        self.W2_post = self.layers.layer_1.linear_act_block.linear.weight.detach().clone().numpy()\n",
    "        self.W3_post = self.layers.layer_2.linear_act_block.linear.weight.detach().clone().numpy()\n",
    "        # self.u_post, self.s_post, self.vh_post = np.linalg.svd(self.W1_post)\n",
    "        # self.W1_post_svd = self.W1_post @ self.vh_post.T\n",
    "    \n",
    "    @staticmethod\n",
    "    def norm(model):\n",
    "        norm = 0\n",
    "        for param in model.parameters():\n",
    "            norm += param.data.norm(2) ** 2\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training loops utils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, crit, opt, n_iters, seed_traj=0, clip_value=1.0):\n",
    "    # Set the seed.\n",
    "    seedall(seed_traj)\n",
    "\n",
    "    model.train()\n",
    "    pbar = tqdm(range(n_iters))\n",
    "    iters_idx, epoch_idx = 0, 0\n",
    "    losses, irelnorms = [], []\n",
    "\n",
    "    while iters_idx < n_iters:\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            if iters_idx >= n_iters: break\n",
    "\n",
    "            # forward pass.\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # backward pass.\n",
    "            opt.zero_grad()\n",
    "            loss = crit(outputs, targets)\n",
    "            loss.backward()\n",
    "            if clip_value > 0:\n",
    "                clip_grad_norm_(model.parameters(), clip_value)\n",
    "            opt.step()\n",
    "            losses.append(loss.item())\n",
    "            irelnorms.append(np.linalg.norm(model.layers.layer_0.linear_act_block.linear.weight.detach().numpy()[:,r:d]))\n",
    "\n",
    "            # move progress bar.\n",
    "            pbar.set_description(f\"epoch {epoch_idx+1} iter {iters_idx+1}/{n_iters} | train loss {loss.item():.3f} | norm {model.norm(model):.3f}\")\n",
    "            pbar.update(1)\n",
    "            iters_idx += 1\n",
    "\n",
    "        epoch_idx += 1\n",
    "\n",
    "    model.set_post()\n",
    "    model.irelnorms = irelnorms\n",
    "    model.losses = losses\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## magic functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(bs, lr, wd, hiddens, n_iters, with_relu=False, seed_traj=0):\n",
    "    # set the init seed.\n",
    "    seedall(seed_init)\n",
    "    \n",
    "    # create dataloader.\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.TensorDataset(xt, yt),\n",
    "        batch_size=bs if bs > 0 else n*2, # set bs = -1 for GD.\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # set model\n",
    "    model = MLP(\n",
    "        in_features=d,\n",
    "        hidden_features=hiddens,\n",
    "        out_features=1,\n",
    "        act_func=ReLU() if with_relu else None,\n",
    "        bias=False\n",
    "    )\n",
    "\n",
    "    # set optim and train.\n",
    "    opt = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    crit = torch.nn.MSELoss()\n",
    "    train(model, dataloader, crit, opt, n_iters, seed_traj)\n",
    "\n",
    "    return model # model contains all infos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gd = experiment(-1, lr, 0.0, hiddens, n_iters=n_iters_gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sgd512 = []\n",
    "for seed_traj in range(n_trajs):\n",
    "    print(f'seeding trajectory {seed_traj}')\n",
    "    model_sgd512.append(experiment(512, lr, 0.0, hiddens, n_iters=n_iters_sgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sgd128 = []\n",
    "for seed_traj in range(n_trajs):\n",
    "    print(f'seeding trajectory {seed_traj}')\n",
    "    model_sgd128.append(experiment(128, lr, 0.0, hiddens, n_iters=n_iters_sgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sgd32 = []\n",
    "for seed_traj in range(n_trajs):\n",
    "    print(f'seeding trajectory {seed_traj}')\n",
    "    model_sgd32.append(experiment(32, lr, 0.0, hiddens, n_iters=n_iters_sgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sgd01 = []\n",
    "for seed_traj in range(n_trajs):\n",
    "    print(f'seeding trajectory {seed_traj}')\n",
    "    model_sgd01.append(experiment(1, lr, 0.0, hiddens, n_iters=n_iters_sgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gdwd = experiment(-1, lr, 0.1, hiddens, n_iters=n_iters_gdwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gram matrices of first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1_gdwd = model_gdwd.W1_post\n",
    "G1_gdwd = abs(W1_gdwd.T @ W1_gdwd)\n",
    "plt.imshow(abs(G1_gdwd), cmap='magma', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1_init = model_gd.W1_init\n",
    "W1_gd = model_gd.W1_post\n",
    "W1_sgd512 = model_sgd512[0].W1_post\n",
    "W1_sgd128 = model_sgd128[0].W1_post\n",
    "W1_sgd32 = model_sgd32[0].W1_post\n",
    "W1_sgd01 = model_sgd01[0].W1_post\n",
    "W1_gdwd = model_gdwd.W1_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G1_init = abs(W1_init.T @ W1_init)\n",
    "G1_gd = abs(W1_gd.T @ W1_gd)\n",
    "G1_sgd512 = abs(W1_sgd512.T @ W1_sgd512)\n",
    "G1_sgd128 = abs(W1_sgd128.T @ W1_sgd128)\n",
    "G1_sgd32 = abs(W1_sgd32.T @ W1_sgd32)\n",
    "G1_sgd01 = abs(W1_sgd01.T @ W1_sgd01)\n",
    "G1_gdwd = abs(W1_gdwd.T @ W1_gdwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs1 = [G1_init, G1_gd, G1_sgd512, G1_sgd128, G1_sgd32, G1_sgd01, G1_gdwd]\n",
    "titles = ['Gram W1 at init.', 'GD', 'SGD-512', 'SGD-128', 'SGD-32', 'SGD-01', 'GD+wd (Gold)']\n",
    "\n",
    "def minmax(x):\n",
    "    return (x - x.min()) / (x.max() - x.min())\n",
    "\n",
    "gs1 = [minmax(g ** 0.95) for g in gs1]\n",
    "vmin = min([g.min() for g in gs1])\n",
    "vmax = max([g.max() for g in gs1])\n",
    "\n",
    "fig, axs = plt.subplots(1, len(titles), figsize=(16, len(gs1)))\n",
    "for i, g in enumerate(gs1):\n",
    "    im = axs[i].imshow(g, cmap='magma', interpolation='nearest', vmin=vmin, vmax=vmax)\n",
    "    axs[i].set_title(f\"{titles[i]}\")\n",
    "    axs[i].set_xticks([])\n",
    "    axs[i].set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws1 = [W1_init, W1_gd, W1_sgd512, W1_sgd32, W1_sgd01]\n",
    "gs1 = [G1_init, G1_gd, G1_sgd512, G1_sgd32, G1_sgd01]\n",
    "wtitles = ['W1 at init.', 'GD', 'SGD-512', 'SGD-32', 'SGD-01']\n",
    "gtitles = ['Gram W1 at init.', 'GD', 'SGD-512', 'SGD-32', 'SGD-01']\n",
    "\n",
    "def minmax(x):\n",
    "    return (x - x.min()) / (x.max() - x.min())\n",
    "\n",
    "fig, axs = plt.subplots(2, len(wtitles), figsize=(12, 6))\n",
    "\n",
    "for i, w in enumerate(ws1):\n",
    "    im = axs[0, i].imshow(abs(w), cmap='viridis', interpolation='nearest')\n",
    "    axs[0, i].set_title(f\"{wtitles[i]}\")\n",
    "    axs[0, i].set_xticks([])\n",
    "    axs[0, i].set_yticks([])\n",
    "\n",
    "gs1 = [minmax(g ** 0.95) for g in gs1]\n",
    "vmin = min([g.min() for g in gs1])\n",
    "vmax = max([g.max() for g in gs1])\n",
    "for i, g in enumerate(gs1):\n",
    "    im = axs[1, i].imshow(g, cmap='magma', interpolation='nearest', vmin=vmin, vmax=vmax)\n",
    "    axs[1, i].set_title(f\"{gtitles[i]}\")\n",
    "    axs[1, i].set_xticks([])\n",
    "    axs[1, i].set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# irelnorms lines :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_median_and_percentiles(data, label, color):\n",
    "    data = np.array(data)\n",
    "    median = np.median(data, axis=0)\n",
    "    percentile_10 = np.percentile(data, 10, axis=0)\n",
    "    percentile_90 = np.percentile(data, 90, axis=0)\n",
    "    plt.plot(median, label=label, color=color)\n",
    "    plt.fill_between(range(len(median)), percentile_10, percentile_90, color=color, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irelnorms_gd = model_gd.irelnorms\n",
    "irelnorms_sgd512 = [model_sgd512[t].irelnorms for t in range(n_trajs)]\n",
    "irelnorms_sgd128 = [model_sgd128[t].irelnorms for t in range(n_trajs)]\n",
    "irelnorms_sgd32 = [model_sgd32[t].irelnorms for t in range(n_trajs)]\n",
    "irelnorms_sgd01 = [model_sgd01[t].irelnorms for t in range(n_trajs)]\n",
    "irelnorms_gdwd = model_gdwd.irelnorms\n",
    "\n",
    "# pad irelnorms_gdwd with 0s to match irelnorms_sgd512 len\n",
    "irelnorms_gdwd = irelnorms_gdwd + [0] * (len(irelnorms_sgd512[0]) - len(irelnorms_gdwd))\n",
    "irelnorms_gd = irelnorms_gd + [irelnorms_gd[-1]] * (len(irelnorms_sgd512[0]) - len(irelnorms_gd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(irelnorms_gd, label='gd', color='red') # blue\n",
    "plot_median_and_percentiles(irelnorms_sgd512, 'sgd-512', 'orange')\n",
    "plot_median_and_percentiles(irelnorms_sgd128, 'sgd-128', 'blue')\n",
    "plot_median_and_percentiles(irelnorms_sgd32, 'sgd-32', 'green')\n",
    "plot_median_and_percentiles(irelnorms_sgd01, 'sgd-01', 'purple')\n",
    "plt.plot(irelnorms_gdwd, label='gd+wd', color='brown') # brown\n",
    "plt.title('irrelevant feature/weights norms over iters.')\n",
    "plt.grid(True, linestyle='-', linewidth=0.2, color='gray')\n",
    "plt.legend()\n",
    "plt.xscale('log')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gram matrices of second layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2_init = model_gd.W2_init\n",
    "W2_gd = model_gd.W2_post\n",
    "W2_sgd512 = model_sgd512[0].W2_post\n",
    "W2_sgd128 = model_sgd128[0].W2_post\n",
    "W2_sgd32 = model_sgd32[0].W2_post\n",
    "W2_sgd01 = model_sgd01[0].W2_post\n",
    "W2_gdwd = model_gdwd.W2_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G2_init = abs(W2_init.T @ W2_init)\n",
    "G2_gd = abs(W2_gd.T @ W2_gd)\n",
    "G2_sgd512 = abs(W2_sgd512.T @ W2_sgd512)\n",
    "G2_sgd128 = abs(W2_sgd128.T @ W2_sgd128)\n",
    "G2_sgd32 = abs(W2_sgd32.T @ W2_sgd32)\n",
    "G2_sgd01 = abs(W2_sgd01.T @ W2_sgd01)\n",
    "G2_gdwd = abs(W2_gdwd.T @ W2_gdwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs2 = [G2_init, G2_gd, G2_sgd512, G2_sgd128, G2_sgd32, G2_sgd01, G2_gdwd]\n",
    "titles = ['Gram W2 at init.', 'GD', 'SGD-512', 'SGD-128', 'SGD-32', 'SGD-01', 'GD+wd (Gold)']\n",
    "\n",
    "def minmax(x):\n",
    "    return (x - x.min()) / (x.max() - x.min())\n",
    "\n",
    "gs2 = [minmax(g ** 0.95) for g in gs2]\n",
    "vmin = min([g.min() for g in gs2])\n",
    "vmax = max([g.max() for g in gs2])\n",
    "\n",
    "fig, axs = plt.subplots(1, len(titles), figsize=(16, len(gs2)))\n",
    "for i, g in enumerate(gs2):\n",
    "    im = axs[i].imshow(g, cmap='magma', interpolation='nearest', vmin=vmin, vmax=vmax)\n",
    "    axs[i].set_title(f\"{titles[i]}\")\n",
    "    axs[i].set_xticks([])\n",
    "    axs[i].set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gram matrices of third layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W3_init = model_gd.W3_init\n",
    "W3_gd = model_gd.W3_post\n",
    "W3_sgd512 = model_sgd512[0].W3_post\n",
    "W3_sgd128 = model_sgd128[0].W3_post\n",
    "W3_sgd32 = model_sgd32[0].W3_post\n",
    "W3_sgd01 = model_sgd01[0].W3_post\n",
    "W3_gdwd = model_gdwd.W3_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G3_init = abs(W3_init.T @ W3_init)\n",
    "G3_gd = abs(W3_gd.T @ W3_gd)\n",
    "G3_sgd512 = abs(W3_sgd512.T @ W3_sgd512)\n",
    "G3_sgd128 = abs(W3_sgd128.T @ W3_sgd128)\n",
    "G3_sgd32 = abs(W3_sgd32.T @ W3_sgd32)\n",
    "G3_sgd01 = abs(W3_sgd01.T @ W3_sgd01)\n",
    "G3_gdwd = abs(W3_gdwd.T @ W3_gdwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs3 = [G3_init, G3_gd, G3_sgd512, G3_sgd128, G3_sgd32, G3_sgd01, G3_gdwd]\n",
    "titles = ['Gram W2 at init.', 'GD', 'SGD-512', 'SGD-128', 'SGD-32', 'SGD-01', 'GD+wd (Gold)']\n",
    "\n",
    "def minmax(x):\n",
    "    return (x - x.min()) / (x.max() - x.min())\n",
    "\n",
    "gs3 = [minmax(g ** 0.95) for g in gs3]\n",
    "vmin = min([g.min() for g in gs3])\n",
    "vmax = max([g.max() for g in gs3])\n",
    "\n",
    "fig, axs = plt.subplots(1, len(titles), figsize=(16, len(gs3)))\n",
    "for i, g in enumerate(gs3):\n",
    "    im = axs[i].imshow(g, cmap='magma', interpolation='nearest', vmin=vmin, vmax=vmax)\n",
    "    axs[i].set_title(f\"{titles[i]}\")\n",
    "    axs[i].set_xticks([])\n",
    "    axs[i].set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesiscode-yovy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
